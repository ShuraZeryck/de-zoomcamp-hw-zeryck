{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07de9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# import pandas as pd\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca5bbb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/28 10:08:18 WARN Utils: Your hostname, shura-tower resolves to a loopback address: 127.0.1.1; using 10.0.1.7 instead (on interface wlo1)\n",
      "22/02/28 10:08:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/shura/spark/spark-3.0.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/02/28 10:08:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3cab876",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhvhv/2021/02/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ccc7d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.registerTempTable('fhvhv_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815be161",
   "metadata": {},
   "source": [
    "##### Question 3. Count records\n",
    "\n",
    "How many taxi trips were there on February 15?\n",
    "\n",
    "Consider only trips that started on February 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "993110f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  367170|\n",
      "+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 17:=====================================>                  (16 + 8) / 24]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    count(1)\n",
    "FROM\n",
    "    fhvhv_data\n",
    "WHERE\n",
    "    DATE(pickup_datetime) = '2021-02-15' \n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58341912",
   "metadata": {},
   "source": [
    "##### Question 4. Longest trip for each day\n",
    "\n",
    "Now calculate the duration for each trip.\n",
    "\n",
    "Trip starting on which day was the longest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e52535d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|      date|duration|\n",
      "+----------+--------+\n",
      "|2021-02-11|   75540|\n",
      "|2021-02-17|   57221|\n",
      "|2021-02-20|   44039|\n",
      "|2021-02-03|   40653|\n",
      "|2021-02-19|   37577|\n",
      "|2021-02-25|   35010|\n",
      "|2021-02-20|   34806|\n",
      "|2021-02-18|   34612|\n",
      "|2021-02-18|   34555|\n",
      "|2021-02-10|   34169|\n",
      "+----------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:======================================================>  (23 + 1) / 24]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    DATE(pickup_datetime) AS date,\n",
    "    (unix_timestamp(dropoff_datetime) - unix_timestamp(pickup_datetime)) AS duration\n",
    "FROM\n",
    "    fhvhv_data\n",
    "ORDER BY\n",
    "    duration DESC\n",
    "LIMIT\n",
    "    10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaf9639",
   "metadata": {},
   "source": [
    "##### Question 5. Most frequent dispatching_base_num\n",
    "\n",
    "Now find the most frequently occurring dispatching_base_num in this dataset.\n",
    "\n",
    "How many stages this spark job has?\n",
    "\n",
    "(Note: the answer may depend on how you write the query, so there are multiple correct answers. Select the one you have.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49fc8cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|dispatching_base_num|count(1)|\n",
      "+--------------------+--------+\n",
      "|              B02876|  215693|\n",
      "|              B03136|    1741|\n",
      "|              B02877|  198938|\n",
      "|              B02869|  429720|\n",
      "|              B02883|  251617|\n",
      "|              B02835|  189031|\n",
      "|              B02884|  244963|\n",
      "|              B02880|  115716|\n",
      "|              B02878|  305185|\n",
      "|              B02836|  128978|\n",
      "|              B02872|  882689|\n",
      "|              B02512|   41043|\n",
      "|              B02867|  200530|\n",
      "|              B02866|  311089|\n",
      "|              B02871|  312364|\n",
      "|              B02889|  138762|\n",
      "|              B02844|    3502|\n",
      "|              B02510| 3233664|\n",
      "|              B02888|  169167|\n",
      "|              B02682|  303255|\n",
      "+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    dispatching_base_num, COUNT(1)\n",
    "FROM\n",
    "    fhvhv_data\n",
    "GROUP BY\n",
    "    dispatching_base_num\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3349bfbe",
   "metadata": {},
   "source": [
    "##### Question 6. Most common locations pair\n",
    "\n",
    "Find the most common pickup-dropoff pair.\n",
    "\n",
    "For example:\n",
    "\n",
    "\"Jamaica Bay / Clinton East\"\n",
    "\n",
    "Enter two zone names separated by a slash\n",
    "\n",
    "If any of the zone names are unknown (missing), use \"Unknown\". For example, \"Unknown / Clinton East\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "30c8c8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-02-23 11:51:05--  https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.199.232\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.199.232|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12322 (12K) [application/octet-stream]\n",
      "Saving to: ‘taxi+_zone_lookup.csv’\n",
      "\n",
      "taxi+_zone_lookup.c 100%[===================>]  12.03K  --.-KB/s    in 0.002s  \n",
      "\n",
      "2022-02-23 11:51:06 (5.86 MB/s) - ‘taxi+_zone_lookup.csv’ saved [12322/12322]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://s3.amazonaws.com/nyc-tlc/misc/taxi+_zone_lookup.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ad62804d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zone_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi+_zone_lookup.csv')\n",
    "\n",
    "zone_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f9cb13c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_zone = types.StructType([\n",
    "    types.StructField('LocationID', types.IntegerType(), True),\n",
    "    types.StructField('Borough', types.StringType(), True),\n",
    "    types.StructField('Zone', types.StringType(), True),\n",
    "    types.StructField('service_zone', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e226dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema_zone) \\\n",
    "    .csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dac64659",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_df.registerTempTable('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "adcccf4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         pickup_zone|        dropoff_zone|\n",
      "+--------------------+--------------------+\n",
      "|      Yorkville West|     Lenox Hill East|\n",
      "|        Baisley Park|     Hammels/Arverne|\n",
      "|Williamsbridge/Ol...|Williamsbridge/Ol...|\n",
      "| Crown Heights North|          Ocean Hill|\n",
      "|   East Harlem North|   East Harlem North|\n",
      "|     Hammels/Arverne|        Far Rockaway|\n",
      "|           Rego Park|            Woodside|\n",
      "| Crown Heights North|Prospect-Lefferts...|\n",
      "|Spuyten Duyvil/Ki...|Upper East Side N...|\n",
      "|Marine Park/Mill ...|Flatbush/Ditmas Park|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COALESCE(pu_zone.Zone, 'Unknown') AS pickup_zone,\n",
    "    COALESCE(do_zone.Zone, 'Unknown') AS dropoff_zone\n",
    "FROM\n",
    "    fhvhv_data AS fhv\n",
    "INNER JOIN\n",
    "    zones as pu_zone\n",
    "ON\n",
    "    fhv.PULocationID = pu_zone.LocationID\n",
    "INNER JOIN\n",
    "    zones as do_zone\n",
    "ON\n",
    "    fhv.DOLocationID = do_zone.LocationID\n",
    "\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6e5e327c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 66:=====================================>                  (16 + 8) / 24]\r",
      "\r",
      "[Stage 66:=================================================>      (21 + 3) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-----+\n",
      "|pu_do_pair                               |count|\n",
      "+-----------------------------------------+-----+\n",
      "|East New York / East New York            |45041|\n",
      "|Borough Park / Borough Park              |37329|\n",
      "|Canarsie / Canarsie                      |28026|\n",
      "|Crown Heights North / Crown Heights North|25976|\n",
      "|Bay Ridge / Bay Ridge                    |17934|\n",
      "+-----------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    CONCAT(t.pickup_zone, ' / ', t.dropoff_zone) AS pu_do_pair,\n",
    "    COUNT(1) as count\n",
    "FROM\n",
    "    (SELECT\n",
    "        COALESCE(pu_zone.Zone, 'Unknown') AS pickup_zone,\n",
    "        COALESCE(do_zone.Zone, 'Unknown') AS dropoff_zone\n",
    "    FROM\n",
    "        fhvhv_data AS fhv\n",
    "    INNER JOIN\n",
    "        zones as pu_zone\n",
    "    ON\n",
    "        fhv.PULocationID = pu_zone.LocationID\n",
    "    INNER JOIN\n",
    "        zones as do_zone\n",
    "    ON\n",
    "        fhv.DOLocationID = do_zone.LocationID\n",
    "    ) AS t\n",
    "GROUP BY\n",
    "    pu_do_pair\n",
    "ORDER BY\n",
    "    count DESC\n",
    "LIMIT\n",
    "    10\n",
    "\"\"\").show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f456f94d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
